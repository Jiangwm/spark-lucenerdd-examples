{
  "paragraphs": [
    {
      "text": "// Add this in the interpreter\n// %dep\n// z.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\n// z.load(\"org.zouzias:spark-lucenerdd_2.11:0.2.8\")\n\nval citiesDF \u003d sqlContext.read.parquet(\"s3://recordlinkage/world-cities-maxmind.parquet\")\ncitiesDF.cache\nval total \u003d citiesDF.count\n\nprintln(s\"Cities: ${total}\")",
      "dateUpdated": "May 31, 2017 6:25:48 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "enabled": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {},
              "map": {
                "baseMapType": "Streets",
                "isOnline": true,
                "pinCols": []
              }
            }
          }
        ],
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "citiesDF: org.apache.spark.sql.DataFrame \u003d [Country: string, City: string ... 5 more fields]\nres1: citiesDF.type \u003d [Country: string, City: string ... 5 more fields]\ntotal: Long \u003d 3173958\nCities: 3173958\n"
          }
        ]
      },
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "citiesDF: org.apache.spark.sql.DataFrame \u003d [Country: string, City: string ... 5 more fields]\nres1: citiesDF.type \u003d [Country: string, City: string ... 5 more fields]\ntotal: Long \u003d 3173958\nCities: 3173958\n"
      },
      "apps": [],
      "jobName": "paragraph_1477339635897_-1053736414",
      "id": "20161017-201243_2050124200",
      "dateCreated": "Oct 24, 2016 8:07:15 AM",
      "dateStarted": "Dec 20, 2016 7:44:44 AM",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.zouzias.spark.lucenerdd.facets.FacetedLuceneRDD\nimport org.apache.spark.sql.DataFrame\n\nval fieldName \u003d \"Country\"\nval k: Int \u003d 20\n\ncitiesDF.cache()\ncitiesDF.count()\nval luceneRDD \u003d FacetedLuceneRDD(citiesDF)\nluceneRDD.cache()\nluceneRDD.count()\n    \ndef benchmark(citiesDF: DataFrame, luceneRDD: FacetedLuceneRDD, k: Int): (Long, Long) \u003d {\n    // Run DataFrame groupBy count and sort\n   \n    val dfStart \u003d System.currentTimeMillis()\n    val dfResults \u003d citiesDF.groupBy(fieldName).count().sort(desc(\"count\")).take(k)\n    val dfEnd \u003d System.currentTimeMillis()\n    \n    // Run Faceted search\n    val lucStart \u003dSystem.currentTimeMillis()\n    luceneRDD.facetQuery(\"*:*\", fieldName, facetNum \u003d k)\n    val lucEnd \u003dSystem.currentTimeMillis()\n    \n    (dfEnd - dfStart, lucEnd - lucStart)\n}",
      "dateUpdated": "Dec 20, 2016 7:44:44 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {},
              "map": {
                "baseMapType": "Streets",
                "isOnline": true,
                "pinCols": []
              }
            }
          }
        ]
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.zouzias.spark.lucenerdd.facets.FacetedLuceneRDD\nimport org.apache.spark.sql.DataFrame\nfieldName: String \u003d Country\nk: Int \u003d 20\nbenchmark: (citiesDF: org.apache.spark.sql.DataFrame, k: Int)(Long, Long)\n"
          }
        ]
      },
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.zouzias.spark.lucenerdd.facets.FacetedLuceneRDD\nimport org.apache.spark.sql.DataFrame\nfieldName: String \u003d Country\nk: Int \u003d 20\nbenchmark: (citiesDF: org.apache.spark.sql.DataFrame, k: Int)(Long, Long)\n"
      },
      "apps": [],
      "jobName": "paragraph_1477339635908_-1070280617",
      "id": "20161017-201314_978365077",
      "dateCreated": "Oct 24, 2016 8:07:15 AM",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val results \u003d List(3, 5).map(x \u003d\u003e benchmark(citiesDF,x))\n\n\nresults.foreach{ case (dfTime, lucTime) \u003d\u003e\n    println(\"\u003d\" * 20)\n    println(s\"DF time: ${ dfTime / 1000D } seconds\")\n    println(s\"Lucene time: ${lucTime / 1000D} seconds\")\n    println(\"\u003d\" * 20)\n}\n",
      "dateUpdated": "Dec 20, 2016 7:44:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {},
              "map": {
                "baseMapType": "Streets",
                "isOnline": true,
                "pinCols": []
              }
            }
          }
        ]
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job 7 cancelled part of cancelled job group zeppelin-20161017-201519_1499660531\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1389)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1638)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n  at org.zouzias.spark.lucenerdd.LuceneRDD.count(LuceneRDD.scala:249)\n  at benchmark(\u003cconsole\u003e:46)\n  at $anonfun$1.apply(\u003cconsole\u003e:37)\n  at $anonfun$1.apply(\u003cconsole\u003e:37)\n  at scala.collection.immutable.List.map(List.scala:273)\n  ... 46 elided\n"
          }
        ]
      },
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job 7 cancelled part of cancelled job group zeppelin-20161017-201519_1499660531\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1389)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1638)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n  at org.zouzias.spark.lucenerdd.LuceneRDD.count(LuceneRDD.scala:249)\n  at benchmark(\u003cconsole\u003e:46)\n  at $anonfun$1.apply(\u003cconsole\u003e:37)\n  at $anonfun$1.apply(\u003cconsole\u003e:37)\n  at scala.collection.immutable.List.map(List.scala:273)\n  ... 46 elided\n"
      },
      "apps": [],
      "jobName": "paragraph_1477339635913_-1072204361",
      "id": "20161017-201519_1499660531",
      "dateCreated": "Oct 24, 2016 8:07:15 AM",
      "status": "ABORT",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Dec 20, 2016 7:44:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "results": []
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "apps": [],
      "jobName": "paragraph_1477339635914_-1071050115",
      "id": "20161017-204445_262828165",
      "dateCreated": "Oct 24, 2016 8:07:15 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Max mind cities",
  "id": "2BZST6GXQ",
  "angularObjects": {
    "2CH4UDTUV:shared_process": [],
    "2CKCFBMPR:shared_process": [],
    "2CJP67NJX:shared_process": [],
    "2CK9YG3TF:shared_process": [],
    "2CK17QQGN:shared_process": [],
    "2CJWNB4BE:shared_process": [],
    "2CH4QY2BC:shared_process": [],
    "2CH3YEQJH:shared_process": [],
    "2CJMFN7AB:shared_process": []
  },
  "config": {},
  "info": {}
}